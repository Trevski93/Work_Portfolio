{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up your device \n",
    "cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up random seed to 1008. Do not change the random seed.\n",
    "# Yes, these are all necessary when you run experiments!\n",
    "seed = 1008\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if cuda:\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data: MNIST\n",
    "#### Load the MNIST training and test dataset using $\\texttt{torch.utils.data.DataLoader}$ and $\\texttt{torchvision.datasets}$. \n",
    "\n",
    "Hint: You might find Alf's notebook useful: https://github.com/Atcold/pytorch-Deep-Learning/blob/master/06-convnet.ipynb, or see some of the PyTorch tutorials."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Load Training Set [4 pts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the MNIST training set with batch size 128, apply data shuffling and normalization\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                    ])),\n",
    "    batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Load Test Set [4 pts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the MNIST test set with batch size 128, apply data shuffling and normalization\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=False,download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                    ])),\n",
    "    batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Models\n",
    "#### You are going to define two convolutional neural networks which are trained to classify MNIST digits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. CNN without Batch Norm [5 pts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in the values below that make this network valid for MNIST data\n",
    "\n",
    "conv1_in_ch = 1\n",
    "conv2_in_ch = 20\n",
    "fc1_in_features = 800\n",
    "fc2_in_features = 500\n",
    "n_classes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetWithoutBatchNorm(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NetWithoutBatchNorm, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=conv1_in_ch, out_channels=20, kernel_size=5, stride=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=conv2_in_ch, out_channels=50, kernel_size=5, stride=1)\n",
    "        self.fc1 = nn.Linear(in_features=fc1_in_features, out_features=500)\n",
    "        self.fc2 = nn.Linear(in_features=fc2_in_features, out_features=n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, kernel_size=2, stride=2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, kernel_size=2, stride=2)\n",
    "        x = x.view(-1, fc1_in_features) # reshaping\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        # Return the log_softmax of x.\n",
    "        return F.log_softmax(x,dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. CNN with Batch Norm [5 pts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in the values below that make this network valid for MNIST data\n",
    "\n",
    "conv1_bn_size = conv2_in_ch\n",
    "conv2_bn_size = 50\n",
    "fc1_bn_size = fc2_in_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the CNN with architecture explained in Part 2.2\n",
    "class NetWithBatchNorm(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NetWithBatchNorm, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=conv1_in_ch, out_channels=20, kernel_size=5, stride=1)\n",
    "        self.conv1_bn = nn.BatchNorm2d(conv1_bn_size)\n",
    "        self.conv2 = nn.Conv2d(in_channels=conv2_in_ch, out_channels=50, kernel_size=5, stride=1)\n",
    "        self.conv2_bn = nn.BatchNorm2d(conv2_bn_size)\n",
    "        self.fc1 = nn.Linear(in_features=fc1_in_features, out_features=500)\n",
    "        self.fc1_bn = nn.BatchNorm1d(fc1_bn_size)\n",
    "        self.fc2 = nn.Linear(in_features=fc2_in_features, out_features=n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1_bn(self.conv1(x)))\n",
    "        x = F.max_pool2d(x, kernel_size=2, stride=2)\n",
    "        x = F.relu(self.conv2_bn(self.conv2(x)))\n",
    "        x = F.max_pool2d(x, kernel_size=2, stride=2)\n",
    "        x = x.view(-1, fc1_in_features)\n",
    "        x = F.relu(self.fc1_bn(self.fc1(x)))\n",
    "        x = self.fc2(x)\n",
    "        # Return the log_softmax of x.\n",
    "        return F.log_softmax(x,dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training & Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Define training method [10 pts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, epoch, log_interval = 100):\n",
    "    # Set model to training mode\n",
    "    model.train()\n",
    "    # Loop through data points\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "    \n",
    "        # Send data and target to device\n",
    "        # TODO\n",
    "        data = data.to(device=device)\n",
    "        target = target.to(device=device,dtype=torch.long)\n",
    "        \n",
    "        # Zero out the ortimizer\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Pass data through model\n",
    "        output = model(data)\n",
    "        \n",
    "        # Compute the negative log likelihood loss\n",
    "        loss = F.nll_loss(output,target)\n",
    "        \n",
    "        # Backpropagate loss\n",
    "        loss.backward()\n",
    "        \n",
    "        # Make a step with the optimizer\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Print loss (uncomment lines below once implemented)\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Define test method [10 pts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define test method\n",
    "def test(model, device, test_loader):\n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "    # Variable for the total loss \n",
    "    test_loss = 0\n",
    "    # Counter for the correct predictions\n",
    "    num_correct = 0\n",
    "    \n",
    "    # don't need autograd for eval\n",
    "    with torch.no_grad():\n",
    "        # Loop through data points\n",
    "        for data, target in test_loader:\n",
    "        \n",
    "            # Send data to device\n",
    "            data = data.to(device=device)\n",
    "            target = target.to(device=device, dtype=torch.long)\n",
    "            \n",
    "            # Pass data through model\n",
    "            output = model(data)\n",
    "            \n",
    "            # Compute the negative log likelihood loss with reduction='sum' and add to total test_loss\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()\n",
    "            \n",
    "            # Get predictions from the model for each data point\n",
    "            pred = output.data.max(1,keepdim=True)[1]\n",
    "            \n",
    "            # Add number of correct predictions to total num_correct \n",
    "            num_correct += pred.eq(target.data.view_as(pred)).cpu().sum().item()\n",
    "    \n",
    "    # Compute the average test_loss\n",
    "    avg_test_loss = test_loss / len(test_loader.dataset)\n",
    "    \n",
    "    # Print loss (uncomment lines below once implemented)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        avg_test_loss, num_correct, len(test_loader.dataset),\n",
    "        100. * num_correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Train NetWithoutBatchNorm() [5 pts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.291516\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.231629\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.144679\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.112140\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.096578\n",
      "\n",
      "Test set: Average loss: 0.0920, Accuracy: 9715/10000 (97%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.121055\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.186163\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.100891\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.057935\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.118874\n",
      "\n",
      "Test set: Average loss: 0.0668, Accuracy: 9778/10000 (98%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.097909\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.071000\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.013582\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.035494\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.014610\n",
      "\n",
      "Test set: Average loss: 0.0573, Accuracy: 9831/10000 (98%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.036453\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.037472\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.039908\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.047904\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.028125\n",
      "\n",
      "Test set: Average loss: 0.0398, Accuracy: 9861/10000 (99%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.031169\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.036780\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.025378\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.025897\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.015703\n",
      "\n",
      "Test set: Average loss: 0.0393, Accuracy: 9868/10000 (99%)\n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.032994\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.033310\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.013145\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.012202\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.010804\n",
      "\n",
      "Test set: Average loss: 0.0366, Accuracy: 9881/10000 (99%)\n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.018236\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.040212\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.080469\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.044411\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.018363\n",
      "\n",
      "Test set: Average loss: 0.0330, Accuracy: 9888/10000 (99%)\n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.037663\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.034032\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.006027\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.012497\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.038948\n",
      "\n",
      "Test set: Average loss: 0.0276, Accuracy: 9910/10000 (99%)\n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.079120\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.024227\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.010042\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.006538\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.057709\n",
      "\n",
      "Test set: Average loss: 0.0304, Accuracy: 9900/10000 (99%)\n",
      "\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.011466\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 0.006410\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.009176\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 0.039144\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.033823\n",
      "\n",
      "Test set: Average loss: 0.0282, Accuracy: 9903/10000 (99%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Deifne model and sent to device\n",
    "model = NetWithoutBatchNorm()\n",
    "model.to(device)\n",
    "\n",
    "# Optimizer: SGD with learning rate of 1e-2 and momentum of 0.5\n",
    "optimizer = optim.SGD(model.parameters(),lr =0.02, momentum= 0.5)\n",
    "\n",
    "# Training loop with 10 epochs\n",
    "for epoch in range(1, 10 + 1):\n",
    "\n",
    "    # Train model\n",
    "    train(model,device,train_loader,optimizer,epoch)\n",
    "    \n",
    "    # Test model\n",
    "    test(model,device,test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Train NetWithBatchNorm() [5 pts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.403052\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.122699\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.199911\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.135071\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.085357\n",
      "\n",
      "Test set: Average loss: 0.0538, Accuracy: 9859/10000 (99%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.091993\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.070561\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.070740\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.027198\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.036871\n",
      "\n",
      "Test set: Average loss: 0.0398, Accuracy: 9879/10000 (99%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.031396\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.012027\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.042147\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.020955\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.028266\n",
      "\n",
      "Test set: Average loss: 0.0317, Accuracy: 9908/10000 (99%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.028212\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.023785\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.020696\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.022342\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.023568\n",
      "\n",
      "Test set: Average loss: 0.0289, Accuracy: 9902/10000 (99%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.022437\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.013333\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.013107\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.015730\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.012875\n",
      "\n",
      "Test set: Average loss: 0.0267, Accuracy: 9915/10000 (99%)\n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.018343\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.009692\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.032750\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.014949\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.008475\n",
      "\n",
      "Test set: Average loss: 0.0248, Accuracy: 9923/10000 (99%)\n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.003612\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.027591\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.017857\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.003280\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.021784\n",
      "\n",
      "Test set: Average loss: 0.0241, Accuracy: 9922/10000 (99%)\n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.006161\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.028247\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.021704\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.018336\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.006412\n",
      "\n",
      "Test set: Average loss: 0.0224, Accuracy: 9928/10000 (99%)\n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.002324\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.009860\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.004998\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.004218\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.009514\n",
      "\n",
      "Test set: Average loss: 0.0220, Accuracy: 9921/10000 (99%)\n",
      "\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.007109\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 0.002834\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.004660\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 0.004179\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.005256\n",
      "\n",
      "Test set: Average loss: 0.0227, Accuracy: 9931/10000 (99%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define model and sent to device\n",
    "model = NetWithBatchNorm()\n",
    "model.to(device)\n",
    "\n",
    "# Optimizer: SGD with learning rate of 1e-2 and momentum of 0.5\n",
    "optimizer = optim.SGD(model.parameters(),lr=0.02,momentum=0.5)\n",
    "\n",
    "# Training loop with 10 epochs\n",
    "for epoch in range(1, 10 + 1):\n",
    "    \n",
    "    # Train model\n",
    "    train(model, device, train_loader, optimizer, epoch)\n",
    "    \n",
    "    # Test model\n",
    "    test(model,device,test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Empirically, which of the models achieves higher accuracy faster? [2 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: NetwithBachNorm reaches higher accuracy faster. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
